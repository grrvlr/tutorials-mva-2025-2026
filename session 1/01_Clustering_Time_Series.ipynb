{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0053d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Machine Learning for Time Series (Master MVA)**\n",
    "\n",
    "- TP1, Thursday 16<sup>th</sup> October 2025\n",
    "- [Link to the class material.](http://www.laurentoudre.fr/ast.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d64a8e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, we focus on the task of **clustering time series**. Through several examples, we will highlight the specific challenges that arise from the **temporal structure** of the data. \n",
    "\n",
    "We will address these challenges using several key concepts introduced in the lessons:\n",
    "\n",
    "- Dynamic Time Warping (DTW)\n",
    "\n",
    "- Convolutional Dictionary Learning\n",
    "\n",
    "- Feature Extraction\n",
    "\n",
    "- Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f1593",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e252567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import rgb2hex\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#import a data loader\n",
    "from aeon.datasets import load_classification\n",
    "#dtw computations\n",
    "from dtw import dtw\n",
    "#compare computation times\n",
    "import time\n",
    "#hierarchical clustering\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import f_oneway \n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import adjusted_rand_score, make_scorer, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from alphacsc import learn_d_z\n",
    "try:\n",
    "    from alphacsc.utils import construct_X\n",
    "except:\n",
    "    from alphacsc.utils.convolution import construct_X\n",
    "\n",
    "from scipy.signal import argrelmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dbc8a",
   "metadata": {},
   "source": [
    "**Utilitary functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distance_matrix_as_table(\n",
    "    distance_matrix, labels=None, figsize=(8, 2)\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.axis(\"tight\")\n",
    "    ax.axis(\"off\")\n",
    "    norm = mpl.colors.Normalize()\n",
    "    cell_colours_hex = np.empty(shape=distance_matrix.shape, dtype=object)\n",
    "    cell_colours_rgba = plt.get_cmap(\"magma\")(norm(distance_matrix))\n",
    "\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "        for j in range(i + 1, distance_matrix.shape[0]):\n",
    "            cell_colours_hex[i, j] = rgb2hex(\n",
    "                cell_colours_rgba[i, j], keep_alpha=True\n",
    "            )\n",
    "            cell_colours_hex[j, i] = cell_colours_hex[i, j]\n",
    "\n",
    "    if labels is not None:\n",
    "        _ = ax.table(\n",
    "            cellText=distance_matrix,\n",
    "            colLabels=labels,\n",
    "            rowLabels=labels,\n",
    "            loc=\"center\",\n",
    "            cellColours=cell_colours_hex,\n",
    "        )\n",
    "    else:\n",
    "        _ = ax.table(\n",
    "            cellText=distance_matrix,\n",
    "            loc=\"center\",\n",
    "            cellColours=cell_colours_hex,\n",
    "        )\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_CDL(signal, Z, D, figsize=(15, 10)):\n",
    "    \"\"\"Plot the learned dictionary `D` and the associated sparse codes `Z`.\n",
    "\n",
    "    `signal` is an univariate signal of shape (n_samples,) or (n_samples, 1).\n",
    "    \"\"\"\n",
    "    (atom_length, n_atoms) = np.shape(D)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.subplot(n_atoms + 1, 3, (2, 3))\n",
    "    plt.plot(signal)\n",
    "    for i in range(n_atoms):\n",
    "        plt.subplot(n_atoms + 1, 3, 3 * i + 4)\n",
    "        plt.plot(D[:, i])\n",
    "        plt.subplot(n_atoms + 1, 3, (3 * i + 5, 3 * i + 6))\n",
    "        plt.plot(Z[:, i])\n",
    "        plt.ylim((np.min(Z), np.max(Z)))\n",
    "\n",
    "def get_n_largest(\n",
    "    arr: np.ndarray, n_largest: int = 3) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Return the n largest values and associated indexes of an array.\n",
    "\n",
    "    (In decreasing order of value.)\n",
    "    \"\"\"\n",
    "    indexes = np.argsort(arr)[-n_largest:][::-1]\n",
    "    if n_largest == 1:\n",
    "        indexes = np.array(indexes)\n",
    "    values = np.take(arr, indexes)\n",
    "    return values, indexes\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def get_largest_local_max(signal1D: np.ndarray, order: int = 1):\n",
    "    \"\"\"Return the largest local max and the associated index in a tuple.\n",
    "\n",
    "    This function uses `order` points on each side to use for the comparison.\n",
    "    \"\"\"\n",
    "    all_local_max_indexes = argrelmax(signal1D, order=order)[0]\n",
    "    all_local_max = np.take(signal1D, all_local_max_indexes)\n",
    "    largest_local_max_index = all_local_max_indexes[all_local_max.argsort()[-1]]\n",
    "\n",
    "    return signal1D[largest_local_max_index], largest_local_max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afce95",
   "metadata": {},
   "source": [
    "# Clustering in Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5085be8",
   "metadata": {},
   "source": [
    "**Clustering** is an unsupervised learning task that aims to **group similar data points** together into clusters, so that objects in the same cluster are more similar to each other than to those in different clusters.\n",
    "\n",
    "### How to define similarity for time series ?\n",
    "\n",
    "For time series, defining a meaningful similarity (or distance) is challenging. Typical issues include:\n",
    "\n",
    "- **Amplitude differences and offset shifts**. *Problem:* Series may have the same shape but different offsets or amplitudes.\n",
    "*Solution:* Normalize series (e.g., z-normalization) to focus on relative shape rather than absolute values.\n",
    "\n",
    "- **Time shifts (temporal misalignment)**. \n",
    "*Problem:* Similar events may occur at slightly different times.\n",
    "*Solution:* Use alignment-based measures such as Dynamic Time Warping (DTW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43db371",
   "metadata": {},
   "source": [
    "# 1. DTW vs Euclidean Distance\n",
    "\n",
    "##  Dataset example\n",
    "\n",
    "FiftyWords is a data set of word outlines taken from the George\n",
    "Washington library by T. Rath and used in the paper \"Word image\n",
    "matching using dynamic time warping\", CVPR 2003.\n",
    "\n",
    "Each case is a word. A series is formed by taking the height\n",
    "profile of the word.\n",
    "\n",
    "![image](alexandria.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609924a",
   "metadata": {},
   "source": [
    "Here, we only deal with the top profile (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbdc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "profiles, labels= load_classification(\"FiftyWords\")\n",
    "profiles=np.squeeze(profiles)\n",
    "\n",
    "# normalize signals (zero mean, unit variance).\n",
    "profiles -= profiles.mean(axis=1).reshape(-1,1)\n",
    "profiles /= profiles.std(axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69420e9",
   "metadata": {},
   "source": [
    "Let's visualize two examples from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1=np.where(labels=='1')[0]\n",
    "word_1, y_1= profiles[labels_1[0]], labels[labels_1[0]]\n",
    "word_2, y_2=profiles[labels_1[3]], labels[labels_1[3]]\n",
    "\n",
    "figsize=(15, 5)\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "ax.plot(word_1, label=y_1)\n",
    "ax.plot(word_2, label=y_2)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9bc47",
   "metadata": {},
   "source": [
    "## DTW between 2 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29147734",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = dtw(word_1, word_2, keep_internals=True)\n",
    "figsize=(15, 5)\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "ax.plot(word_1, label=y_1)\n",
    "ax.plot(word_2, label=y_2)\n",
    "plt.title(f\"DTW: {alignment.distance:.2f}\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9896df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment.plot(type=\"threeway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment.plot(type=\"twoway\", offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4581f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Write a function which computes the DTW distance between two signals using the dtw module: <tt>get_dtw_distance(signal_1: np.ndarray, signal_2: np.ndarray)->float</tt>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtw_distance(signal_1,signal_2):\n",
    "    pass\n",
    "\n",
    "def get_euclidean_distance(signal_1,signal_2):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f08e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Choose one of the two words and plot the most similar and the most dissimilar, according to the DTW. In addition, print the associated labels.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951a5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00750aec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Repeat the same procedure with the Euclidean distance. Compare the results and the computation times with DTW (you may use the time module).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71a458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbd3cf0",
   "metadata": {},
   "source": [
    "## Clustering with DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494de9a5",
   "metadata": {},
   "source": [
    "### Clustering a small subset\n",
    "\n",
    "Out of the whole data set, let us choose 6 word profiles from 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1378c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few profiles with two different classes\n",
    "keep_mask = np.isin(labels, [\"31\", \"34\"])\n",
    "labels_sub = labels[keep_mask]\n",
    "profiles_sub = profiles[keep_mask]\n",
    "# reorder by label\n",
    "order_indexes = labels_sub.argsort()\n",
    "labels_sub = labels_sub[order_indexes]\n",
    "profiles_sub = profiles_sub[order_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dafcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "distance_matrix = np.zeros(\n",
    "    (profiles_sub.shape[0], profiles_sub.shape[0]), dtype=float\n",
    ")\n",
    "\n",
    "for row in range(profiles_sub.shape[0]):\n",
    "    for col in range(row + 1, profiles_sub.shape[0]):\n",
    "        distance_matrix[row, col] = get_dtw_distance(\n",
    "            profiles_sub[row], profiles_sub[col]\n",
    "        )\n",
    "        distance_matrix[col, row] = distance_matrix[row, col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c0f5a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Create the same plot (distance matrix) with the Euclidean distance instead of the DTW. What do you observe?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad53b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distance_matrix=..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = display_distance_matrix_as_table(\n",
    "    np.round(euclidean_distance_matrix, 2), labels=labels_sub\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20910c7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>One of the most popular clustering algorithms is K-Means. What difficulties or limitations might you encounter when trying to use standard K-Means with DTW?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680fe5fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b71498",
   "metadata": {},
   "source": [
    "To overcome these difficulties, we use a hierarchical clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrix using the 'Ward' criterion\n",
    "linkage = hierarchy.ward(distance_matrix)\n",
    "\n",
    "figsize=(20, 10)\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "cut_threshold = 200\n",
    "\n",
    "dendro = hierarchy.dendrogram(\n",
    "    linkage,\n",
    "    ax=ax,\n",
    "    labels=labels_sub,\n",
    "    color_threshold=cut_threshold,\n",
    "    distance_sort=True,\n",
    ")\n",
    "ax.axhline(cut_threshold, ls=\"--\", color=\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cad6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred = hierarchy.fcluster(linkage, t=cut_threshold, criterion='distance')\n",
    "ari=adjusted_rand_score(labels_sub,labels_pred)\n",
    "ari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291cbea",
   "metadata": {},
   "source": [
    "### Clustering on a larger subset\n",
    "\n",
    "Using the DTW, we can cluster a large set of data (43 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ec07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few profiles with two different classes\n",
    "keep_mask = np.isin(labels, [\"4\", \"6\", \"14\"])\n",
    "profiles_sub = profiles[keep_mask]\n",
    "labels_sub = labels[keep_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4448ef31",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Compare the clustering produced by DTW and Euclidean distance by plotting dendrograms, choosing a cut_threshold to form homogeneous clusters, and evaluating the results with the Adjusted Rand Index (ARI). Discuss which distance better captures time series similarity.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ae898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of the previous double for loop, we can use scipy function pdist\n",
    "dtw_distance_matrix = pdist(\n",
    "    profiles_sub.squeeze(), metric=get_dtw_distance\n",
    ")  # condensed distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_threshold = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of the previous double for loop, we can use scipy function pdist\n",
    "euclidean_distance_matrix = pdist(\n",
    "    profiles_sub.squeeze(), metric=get_euclidean_distance\n",
    ")  # condensed distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ab8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_threshold = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723ff40",
   "metadata": {},
   "source": [
    "# 2. Clustering at the scale of atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2664f7c",
   "metadata": {},
   "source": [
    "Up to now, we have considered time series where class differences could be observed at the **global scale** of the signal. However, in many real-world applications the discriminative information is not visible over the whole series but rather at the level of **local patterns (atoms)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9e4dc",
   "metadata": {},
   "source": [
    "## Dataset example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654687f9",
   "metadata": {},
   "source": [
    "Task is to classify the nature of the heartbeat signal.\n",
    "\n",
    "Heart sound recordings were sourced from several contributors around the world, collected at either a clinical or nonclinical environment, from both healthy subjects and pathological patients.\n",
    "Each series represent the amplitude of the signal over time.\n",
    "\n",
    "Heart sound recordings were sourced from several contributors around the world, collected at either a clinical or nonclinical environment, from both healthy subjects and pathological patients.\n",
    "The heart sound recordings were collected from different locations on the body. The typical four locations are aortic area, pulmonic area, tricuspid area and mitral area, but could be one of nine different locations.\n",
    "The sounds were divided into two classes: normal and abnormal. The normal recordings were from healthy subjects and the abnormal ones were from patients with a confirmed cardiac diagnosis.\n",
    "The patients suffer from a variety of illnesses, but typically they are heart valve defects and coronary artery disease patients.\n",
    "Heart valve defects include mitral valve prolapse, mitral regurgitation, aortic stenosis and valvular surgery. All the recordings from the patients were generally labeled as abnormal.\n",
    "Both healthy subjects and pathological patients include both children and adults.\n",
    "\n",
    "Data was recorded at 2,000Hz.\n",
    "Data was truncated to the shortest instance.\n",
    "\n",
    "Original data can be found here:\n",
    "https://www.physionet.org/physiobank/database/challenge/2016/\n",
    "\n",
    "Original paper:\n",
    "Goldberger AL, Amaral LA, Glass L, Hausdorff JM, Ivanov PC, Mark RG, Mietus JE, Moody GB, Peng CK, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation. 2000 Jun 13;101(23):e215-20.\n",
    "\n",
    "Correspondence should be addressed to Ary L. Goldberger:\n",
    "ary@astro.bidmc.harvard.edu\n",
    "\n",
    "Instances: 409\n",
    "\n",
    "Time series length: 18,530\n",
    "\n",
    "Classes:\n",
    "- Normal (110)\n",
    "- Abnormal (299)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y= load_classification(\"BinaryHeartbeat\")\n",
    "X=X.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a592e",
   "metadata": {},
   "source": [
    "We can plot a signal from each class (normal and abnormal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENCY = 2000  # Hz\n",
    "\n",
    "fig, ax_arr = plt.subplots(nrows=1, ncols=2, figsize=(20, 3), sharey=True)\n",
    "fig.tight_layout()\n",
    "for ind, ax in zip([1, 200], ax_arr):\n",
    "    s = X[ind]\n",
    "    tt = np.arange(s.size) / FREQUENCY\n",
    "    ax.plot(tt, s)\n",
    "    ax.set_xlim(0, s.size / FREQUENCY)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    _ = ax.set_title(f\"label: {y[ind]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150ad4b",
   "metadata": {},
   "source": [
    "For the subsequent study, we select only 6 elements (3 from each classe) from the complete data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample = [0, 1, 2, 190, 191, 192]  # 3 Normal, 3 Abnormal\n",
    "sub_X = np.take(\n",
    "    X, sub_sample, axis=0\n",
    ").squeeze()  # shape (n_series, n_samples)\n",
    "sub_y = np.take(y, sub_sample, axis=0)  # shape (n_series,)\n",
    "\n",
    "# normalize signals (zero mean, unit variance).\n",
    "sub_X -= sub_X.mean(axis=1).reshape(-1, 1)\n",
    "sub_X /= sub_X.std(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d04b44",
   "metadata": {},
   "source": [
    "Let's zoom to see in more details the events of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac9468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = [2, 4]  # indices des signaux à afficher\n",
    "fig, ax_arr = plt.subplots(nrows=2, ncols=2, figsize=(20, 6), sharey=True)\n",
    "fig.tight_layout(pad=4)\n",
    "\n",
    "for row, ind in enumerate(inds):\n",
    "    signal = sub_X[ind]\n",
    "    label = sub_y[ind]\n",
    "    n_samples = signal.size\n",
    "    tt = np.arange(n_samples) / FREQUENCY\n",
    "\n",
    "    # Signal complet\n",
    "    ax = ax_arr[row, 0]\n",
    "    ax.plot(tt, signal)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xlim(0, n_samples / FREQUENCY)\n",
    "    ax.set_title(f\"Signal index {ind}, label: {label}\")\n",
    "\n",
    "    # Zoom sur un segment\n",
    "    ax = ax_arr[row, 1]\n",
    "    start, end = 0, 4000  # ajuster selon le zoom souhaité\n",
    "    ax.plot(tt[start:end], signal[start:end])\n",
    "    ax.set_xlim(tt[start], tt[end])\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_title(f\"Zoom on [{start/FREQUENCY:.2f}s, {end/FREQUENCY:.2f}s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dff0c0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Roughly, what is the duration of the important phenomenon (the heartbeat)?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05a6f3",
   "metadata": {},
   "source": [
    "To compare the signals at a finer scale, we need to extract their elementary atoms. One way to achieve this is through Convolutional Dictionary Learning (CDL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69617d90",
   "metadata": {},
   "source": [
    "## CDL on a single signal\n",
    "\n",
    "For a 1D signal $\\mathbf{x}\\in\\mathbb{R}^N$ with $N$ samples, the convolutional dictionary learning tasks amounts to solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{(\\mathbf{d}_k)_k, (\\mathbf{z}_k)_k \\\\ \\lVert\\mathbf{d}_k\\rVert^2\\leq 1} \\quad (1/2)\\left\\lVert \\mathbf{x} - \\sum_{k=1}^K \\mathbf{z}_k * \\mathbf{d}_k \\right\\rVert^2 \\quad + \\quad\\lambda \\sum_{k=1}^K \\lVert\\mathbf{z}_k\\rVert_1\n",
    "$$\n",
    "\n",
    "where $\\mathbf{d}_k\\in\\mathbb{R}^L$ are the $K$ dictionary atoms (patterns), $\\mathbf{z}_k\\in\\mathbb{R}^{N-L+1}$ are activations signals, and $\\lambda>0$ is the sparsity constraint.\n",
    "\n",
    "This problem is not convex with respect to the couple $(\\mathbf{d}_k)_k, (\\mathbf{z}_k)_k$ but convex when the subproblems are taken individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63bc86b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>What are the parameters that a user must calibrate when using CDL?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe1761",
   "metadata": {},
   "source": [
    "We can now apply CDL on a single signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a signal\n",
    "signal = sub_X[2]\n",
    "data = signal[np.newaxis, :]  # shape (1, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5825e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to change\n",
    "n_atoms = 3  # K\n",
    "atom_length = 2000  # L\n",
    "penalty = 4  # lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning a dictionary and codes\n",
    "pobj, _, d_hat, z_hat, _ = learn_d_z(\n",
    "    X=data,\n",
    "    n_atoms=n_atoms,\n",
    "    n_times_atom=atom_length,\n",
    "    reg=penalty,\n",
    "    n_iter=30,\n",
    "    n_jobs=3,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plot_CDL(signal, z_hat.T.squeeze(), d_hat.T.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd671de2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>How does the number of activation evolve when the sparsity penalty changes?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65bcd6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Looking at the sparse codes, can you tell:</p>\n",
    "    <ul>    \n",
    "    <li>How many times each atom is activated?</li>\n",
    "    <li>What is the compression rate (number of non-zero coefficients in the sparse codes / signal length)?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f873771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with the dictionary and the sparse codes\n",
    "reconstruction = construct_X(z_hat, d_hat).squeeze()\n",
    "\n",
    "figsize=(15, 5)\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "tt = np.arange(signal.shape[0])\n",
    "ax.plot(tt, signal, label=\"original\", alpha=0.5)\n",
    "ax.plot(tt, reconstruction, label=\"reconstructed\")\n",
    "\n",
    "ax.set_title(f\"Reconstruction MSE: {np.mean((signal - reconstruction)**2):.2e}\")\n",
    "\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228ea75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>How does reconstruction error evolve when the sparsity penalty changes?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60110299",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(\n",
    "    nrows=n_atoms // 3 + 1,\n",
    "    ncols=3,\n",
    "    figsize=(20, 4 * (n_atoms // 3 + 1)),\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "for k in range(n_atoms):\n",
    "    ax = ax_arr.flatten()[k]\n",
    "    reconstruted_with_one_atom = construct_X(\n",
    "        z_hat[k, np.newaxis, :, :], d_hat[k, np.newaxis, :]\n",
    "    ).squeeze()\n",
    "    ax.plot(range(start, end), signal[start:end])\n",
    "    ax.plot(\n",
    "        range(start, end),\n",
    "        reconstruted_with_one_atom[start:end],\n",
    "    )\n",
    "    ax.set_title(f\"Atom {k} only\")\n",
    "\n",
    "ax = ax_arr.flatten()[n_atoms]\n",
    "ax.plot(range(start, end), signal[start:end])\n",
    "ax.plot(\n",
    "    range(start, end),\n",
    "    reconstruction[start:end],\n",
    ")\n",
    "_ = ax.set_title(f\"All atoms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ace6e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Rerun the dictionary learning and sparse coding. What do you observe on the motif shape? And the reconstruction error?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103305ed",
   "metadata": {},
   "source": [
    "## CDL on the whole data set\n",
    "\n",
    "In this section, we apply CDL on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc56a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following, we fix the number of atoms and their length\n",
    "n_atoms = 5\n",
    "atom_length = 1500\n",
    "penalty = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfffb48",
   "metadata": {},
   "source": [
    "Dictionary learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417acc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pobj, _, d_hat, z_hat, _ = learn_d_z(\n",
    "    X=sub_X,\n",
    "    n_atoms=n_atoms,\n",
    "    n_times_atom=atom_length,\n",
    "    reg=penalty,\n",
    "    verbose=1,\n",
    "    n_jobs=3,\n",
    "    n_iter=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(\n",
    "    nrows=n_atoms // 3 + 1,\n",
    "    ncols=3,\n",
    "    figsize=(20, 4 * (n_atoms // 3 + 1)),\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "for k, (atom, ax) in enumerate(zip(d_hat, ax_arr.flatten())):\n",
    "    ax.plot(atom)\n",
    "    ax.set_xlim(0, atom.size)\n",
    "    ax.set_title(f\"Atom {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, (label, signal) in enumerate(zip(sub_y, sub_X)):\n",
    "    codes = z_hat[:, k, :]\n",
    "    reconstruction = construct_X(codes[:, np.newaxis, :], d_hat).squeeze()\n",
    "    error = np.mean((signal - reconstruction) ** 2)\n",
    "    nnz_activations = (codes > 1e-3).sum()\n",
    "\n",
    "    # select the used atoms\n",
    "    most_used_atoms_activations, most_used_atoms_indexes = get_n_largest(\n",
    "        (codes > 1e-3).sum(axis=1), n_largest=1\n",
    "    )\n",
    "    most_used_atom_msg = \", \".join(\n",
    "        f\"{ind} ({acti*100/nnz_activations:.1f}%)\"\n",
    "        for (acti, ind) in zip(\n",
    "            most_used_atoms_activations, most_used_atoms_indexes\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"Label: {label}, MSE: {error:.2f}, non-zero activations: {nnz_activations}, most used atoms: {most_used_atom_msg}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28e6eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Based on the results, does this approach appear suitable for effectively clustering such time series?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df694030",
   "metadata": {},
   "source": [
    "# 3. Clustering via Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076dd2fe",
   "metadata": {},
   "source": [
    "The final approach we consider in this lab is more closely related to traditional machine learning methods. It consists in extracting a well-chosen set of features from the time series and subsequently applying standard clustering algorithms.\n",
    "\n",
    "\n",
    "In the following, we will use the same dataset as before. Let’s start by examining in detail the different features that can be extracted from an example time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b415e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=0\n",
    "signal=X[0]\n",
    "label = y[ind]\n",
    "n_samples = signal.shape[0]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5ab29",
   "metadata": {},
   "source": [
    "## 3.a. Statistical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006b646e",
   "metadata": {},
   "source": [
    "### Moment and percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386317e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution_features(signal: np.ndarray) -> dict:\n",
    "    res_dict = dict()\n",
    "    res_dict[\"mean\"] = signal.mean()\n",
    "    res_dict[\"std\"] = signal.std()\n",
    "    res_dict[\"min\"] = signal.min()\n",
    "    res_dict[\"max\"] = signal.max()    \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7549379",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution_features(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d894077",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Rewrite in the following cell, the function <tt>get_distribution_features(signal)->dict</tt> so that is also computes the kurtosis (moment of order 4), the skew (measure of the asymmetry of the proba. distrib.) available in the <tt>scipy.stats</tt> module and the 25%, 50% and 75% percentiles.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94deb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def get_distribution_features(signal: np.ndarray) -> dict:\n",
    "    res_dict = dict()  \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e840343",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distribution_features(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd278900",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "\n",
    "For a signal $x\\in\\mathbb{R}^N$ with $N$ samples, the autocorrelation with lag $m$ is defined as follows:\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma}[m] := \\frac{1}{N-|m|} \\sum_{n=0}^{N -|m|-1} x[n]x[n+m].\n",
    "$$\n",
    "\n",
    "Note that in practice, peaks observed in the autocorrelation function correspond to repetitions in the signal, and therefore reveal its underlying periodicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3ff9d",
   "metadata": {},
   "source": [
    "Let us plot the autocorrelation of a sound signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.signal import find_peaks\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(acf(signal, nlags=200, fft=True), \".-\")\n",
    "ax.axhline(0, ls=\"--\", color=\"k\")\n",
    "ax.set_title('Autocorrelation')\n",
    "ax.set_xlabel('Lags')\n",
    "ax.set_ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a60c39",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>We would like to compute the largest positive and negative autocorrelation peaks (excluding the trivial one at lag 0), together with their associated lags in Hz. How can you convert a lag expressed in number of samples into a lag expressed in Hz?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06257ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_hz(lag_in_samples, FREQUENCY):\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11bd3c",
   "metadata": {},
   "source": [
    "The autocorrelation features retained in the end consist of the full set of autocorrelation values, together with the amplitudes of the first negative and positive peaks and their corresponding lags.\n",
    "\n",
    "Compute the autocorrelation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autocorr_features(signal: np.ndarray, n_lags: int = 200) -> dict:\n",
    "    auto_corr = acf(signal, nlags=n_lags, fft=True)\n",
    "    res_dict = dict()\n",
    "    for lag, auto_corr_value in enumerate(auto_corr):\n",
    "        res_dict[f\"autocorrelation_{lag}_lag\"] = auto_corr_value\n",
    "\n",
    "    local_max, local_argmax = get_largest_local_max(auto_corr[1:], order=5)\n",
    "    local_argmax += 1  # to account for the lag=0 removed before\n",
    "    local_min, local_argmin = get_largest_local_max(-auto_corr[1:], order=5)\n",
    "    local_min = -local_min\n",
    "    local_argmin += 1  # to account for the lag=0 removed before\n",
    "    res_dict[\"largest_local_max_autocorrelation\"] = local_max\n",
    "    res_dict[\"lag_largest_local_max_autocorrelation_Hz\"] = samples_to_hz(local_argmax, FREQUENCY)\n",
    "    res_dict[\"largest_local_min_autocorrelation\"] = local_min\n",
    "    res_dict[\"lag_largest_local_min_autocorrelation_Hz\"] = samples_to_hz(local_argmin, FREQUENCY)\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6899e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_autocorr_features(signal, n_lags=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6e5d5",
   "metadata": {},
   "source": [
    "## 3.b Spectral features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93b776",
   "metadata": {},
   "source": [
    "Using the Discrete Fourier Transform (DFT), we can identify which frequencies are present in the signal and their associated amplitudes.\n",
    "\n",
    "For a signal $x\\in\\mathbb{R}^N$ with $N$ samples, the $k$-th discrete fourier coefficient is defined as follows:\n",
    "\n",
    "$$\n",
    "X[k] = \\sum_{n=0}^{N-1}x[n]e^{-j2\\pi\\frac{kn}{N}}.\n",
    "$$\n",
    "$X[k]$ corresponds to the DFT for the physical frequency\n",
    "$$f[k]=k\\frac{F_s}{N}$$\n",
    "\n",
    "\n",
    "Plot the Fourier coefficients of a signal (y=absolute value, x=frequency) using `rfft()` and `rfftfreq()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import rfft, rfftfreq\n",
    "fourier = abs(rfft(signal)) ** 2\n",
    "freqs = rfftfreq(n=n_samples, d=1.0 / FREQUENCY)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(freqs, fourier)\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "_ = ax.set_ylabel(\"Fourier coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ec950",
   "metadata": {},
   "source": [
    "Compute the spectral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870808b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier_features(signal: np.ndarray, n_bins: int = 100) -> dict:\n",
    "    \"\"\"The signal is assumed to be centered and scaled to unit variance.\"\"\"\n",
    "    n_samples = signal.shape[0]\n",
    "    fourier = abs(rfft(signal))\n",
    "    freqs = rfftfreq(n=n_samples, d=1.0 / FREQUENCY)\n",
    "    res_dict = dict()\n",
    "\n",
    "    freq_bins = np.linspace(0, FREQUENCY / 2, n_bins + 1)\n",
    "    for f_min, f_max in pairwise(freq_bins):\n",
    "        keep = (f_min <= freqs) & (freqs < f_max)\n",
    "        res_dict[f\"fourier_{f_min:.0f}-{f_max:.0f}_Hz\"] = np.log(\n",
    "            np.sum(fourier[keep] ** 2)\n",
    "        )\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46615b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fourier_features(signal, n_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df92a73",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>The frequency features are not scaled. If the signal is normalized, do we need to?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2b988",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a406d480",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> We have used regularly spaced frequency bins. What would be a better approach? </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c917ba5",
   "metadata": {},
   "source": [
    "## Concatenate all features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79111773",
   "metadata": {},
   "source": [
    "In the following cell, we write a function `get_features(signal: np.ndarray) -> dict` that computes all features from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(signal: np.ndarray) -> dict:\n",
    "    res_dict = dict()\n",
    "\n",
    "    # stats\n",
    "    res_dict.update(get_distribution_features(signal))\n",
    "\n",
    "    # spectral\n",
    "    signal -= signal.mean()\n",
    "    signal /= signal.std()\n",
    "    res_dict.update(get_fourier_features(signal, n_bins=50))\n",
    "\n",
    "    # autocorrelation\n",
    "    res_dict.update(get_autocorr_features(signal, n_lags=200))\n",
    "\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32843d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7438859",
   "metadata": {},
   "source": [
    "Compute all features over the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.DataFrame(\n",
    "    [get_features(signal) for signal in X]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e398f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df28e2b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> How many features do we have in the end?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb9b98c",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf769e",
   "metadata": {},
   "source": [
    "### Unsupervised selection\n",
    "\n",
    "#### Sanity checks \n",
    "\n",
    "**Low variance threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "all_features.std().sort_values().plot(ax=ax)\n",
    "\n",
    "# change the height of the horizontal line here\n",
    "ax.axhline(0.1, ls=\"--\", color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d8e16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Choose a variance threshold (name it <tt>variance_threshold</tt>).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9d5ec",
   "metadata": {},
   "source": [
    "Be careful when dropping low variance features, they might still be informative. Quickly check that it makes sense to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9926abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance_features = all_features.std() < variance_threshold\n",
    "low_variance_features = low_variance_features[\n",
    "    low_variance_features\n",
    "].index.to_numpy()\n",
    "print(f\"There are {len(low_variance_features)} features to drop.\")\n",
    "print(low_variance_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595d46e",
   "metadata": {},
   "source": [
    "The following cell drops the low-variance features. Only execute it when you are sure of the features you want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.drop(columns=low_variance_features, inplace=True, errors=\"ignore\")\n",
    "print(f\"There are {all_features.shape[1]} features left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78946510",
   "metadata": {},
   "source": [
    "**Check outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce0c32",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Some features have larger variations than others: which ones are they? (Create a list <tt>features_to_check</tt>.) Show the associated histograms.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.std().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d207566",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf35ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_to_check:\n",
    "    #plot histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc21946",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Check the outliers. Is there something we can do to remediate this issue?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82851703",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_to_check:\n",
    "    mean_val = all_features[feature].mean()\n",
    "    std_val = all_features[feature].std()\n",
    "    lower = mean_val - 3 * std_val\n",
    "    upper = mean_val + 3 * std_val\n",
    "\n",
    "    # clip les valeurs en dehors des bornes\n",
    "    all_features[feature] = all_features[feature].clip(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32768f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features_to_check:\n",
    "    all_features[feature].hist(bins=30, figsize=(15, 5))\n",
    "    plt.title(feature)\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a8f6a",
   "metadata": {},
   "source": [
    "**Multicollinearity**\n",
    "\n",
    "Multicollinearity degrades numerical stability and interpretability.\n",
    "Compute the correlation.\n",
    "Features with a correlation above a threshold are grouped together in a cluster.\n",
    "We then choose a single feature from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc684a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9134d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pdist(all_features.to_numpy().T, metric=\"correlation\")  # distance matrix\n",
    "plt.imshow(squareform(corr))\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation distance matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "corr_linkage = hierarchy.average(corr)\n",
    "dendro = hierarchy.dendrogram(\n",
    "    corr_linkage, ax=ax, color_threshold=1 - correlation_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the clusters\n",
    "cluster_ids = hierarchy.fcluster(\n",
    "    corr_linkage, 1 - correlation_threshold, criterion=\"distance\"\n",
    ")\n",
    "\n",
    "# print the largest clusters\n",
    "largest_cluster_ind = np.bincount(cluster_ids).argmax()\n",
    "print(\n",
    "    f\"The largest cluster is {all_features.columns[cluster_ids==largest_cluster_ind].tolist()}.\"\n",
    ")\n",
    "\n",
    "# for each cluster, only keep the first feature\n",
    "keep_features = list()\n",
    "for cluster in np.unique(cluster_ids):\n",
    "    cluster_indexes = np.where(cluster_ids == cluster)[0]\n",
    "    keep_features.append(all_features.columns[cluster_indexes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44edfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of features\n",
    "all_features = all_features[keep_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6aaf48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> How many features do we have in the end?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d744630",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b03c36",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519fa4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_centered = all_features.to_numpy()\n",
    "\n",
    "all_features_centered -= all_features_centered.mean(axis=0)\n",
    "all_features_centered /= all_features_centered.std(axis=0)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2).fit(all_features_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204802c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 5 most important features (with highest norm).\n",
    "top_features_for_pca = np.linalg.norm(pca.components_, axis=0).argsort()[-10:]\n",
    "\n",
    "for feature_ind in top_features_for_pca:\n",
    "    msg = f\"{all_features.columns[feature_ind]}: {(pca.components_.T[feature_ind]**2).sum():.3f}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd156d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2D projection of the top 10 features\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.axis(\"equal\")\n",
    "for feature_ind in top_features_for_pca:  # normaliser par la variance\n",
    "    dx, dy = pca.components_.T[feature_ind]\n",
    "    ax.plot([0, dx], [0, dy], label=all_features.columns[feature_ind])\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_[0]:.1f}% of variance)\")\n",
    "ax.set_ylabel(f\"PC1 ({pca.explained_variance_[1]:.1f}% of variance)\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eec083",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Given a target variance threshold, determine the minimum number of principal components (n_components) required to reach or exceed this level of explained variance using PCA</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fe8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c169804",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p> Using the number of components (n_components) determined previously, rerun PCA with this number of components, transform the dataset into the reduced space, and then apply K-means clustering on the transformed data.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0191830",
   "metadata": {},
   "source": [
    "### Supervised selection\n",
    "\n",
    "In the supervised setting, we can further refine feature selection by taking the labels into account, keeping only the features that are most informative for the prediction task.\n",
    "\n",
    "Three main classes of methods:\n",
    "\n",
    "- **Filter methods.** We test the adequacy of the feature with the annotations, thanks to several criterion/scores (e.g. correlation).\n",
    "- **Wrapper methods.** We test the features on a supervised classification task, by trying several combinations. The best features are kept.\n",
    "- **Embedded methods.** Mixed approaches where we jointly infer the relevance of the features and classify the data (decision tree, sparse methods...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953958ca",
   "metadata": {},
   "source": [
    "**Filter methods**\n",
    "\n",
    "If a feature is highly correlated with a label, it can help achieve good classification performance.\n",
    "\n",
    "For classification tasks, we can use the ANOVA test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y\n",
    "statistics, pvalues = f_oneway(\n",
    "    all_features[labels == \"0\"], all_features[labels == \"1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e12b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in statistics.argsort()[::-1]:\n",
    "    print(f\"{all_features.columns[ind]}: {statistics[ind]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddf0c5",
   "metadata": {},
   "source": [
    "**Wrapper methods**\n",
    "\n",
    "For a given learning algorithm, wrapper methods repeatedly select a subset of features and evaluate the selected features. Several procedures exist to select a subset but the most common are greedy and iterative; they either remove (backward selection) or add a feature (forward selection) sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4750f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At each steps, SequentialFeatureSelector adds the best scoring feature to\n",
    "# the set of selected features.\n",
    "# For a given estimator, the score is computed with cross-validation.\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "sfs = SequentialFeatureSelector(\n",
    "    knn, n_features_to_select=20, cv=5, scoring=make_scorer(accuracy_score)\n",
    ")\n",
    "sfs.fit(all_features, labels)\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(all_features.columns[sfs.get_support()].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119575e",
   "metadata": {},
   "source": [
    "### Embedded methods\n",
    "\n",
    "Such methods use the intrinsic structure of a learning algorithm to embed feature selection into the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(all_features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6a95e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "    <p><b>Question</b></p>\n",
    "    <p>Print features in descending order of importance, measured by the random forest feature importance (see the <tt>feature_importances_</tt> attribute).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949b93ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
